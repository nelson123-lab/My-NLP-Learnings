# NLP
-Process in which a we make a computer language understand, spare and extract human understandable languages. There are different areas of NLP.

## NER ( Named Entity Recognition )
- Part of Speech Tagging (POS)
- Syntactic Parsing
- Text Categorization
- Coreference Resolution
- Machine Translation

## NLU ( Natural Language Understanding )
- Relation Extraction
- Paraphrasing
- Semantic Parsing
- Sentiment Analysis
- Question and Answering
- Summarization

### General Informations

#### POC
A POC (proof of concept) is an advanced demo project that reflects a real-world scenario. Since developing products from emerging technologies can be too risky or troublesome, POCs are often used to “prove” that a new technology, service, or idea is viable for the market.

#### Vision-language Models
A vision-language model typically consists of 3 key elements: an image encoder, a text encoder, and a strategy to fuse information from the two encoders. These key elements are tightly coupled together as the loss functions are designed around both the model architecture and the learning strategy.

#### RLHF (Reinforcement Learning from Human Feedback)
RLHF is a method of using a scalar reward value as a fine-tuning mechanism. Since the reward value can take any form, for example, human ranking or final score in a game, it is usually not differentiable and thus can't be used as a loss function.

## Latest LLM models
1. Llama (Large Language Model Meta AI)
LLaMA's ability to generate natural language text means that it can be used for various applications, from chatbots and virtual assistants to content creation and data analysis. In addition to its massive size and training data, LLaMA also uses various optimization techniques to improve its performance.

2. OPT (Open Pre-trained Transformer)
OPT is a series of open-sourced large causal language models which perform similar in performance to GPT3. The abstract from the paper is the following: Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning.

3. Alpaca
Alpaca is a small but capable 7B language model developed by researchers at Stanford University's Centre for Research on Foundation Models. It was fine-tuned from Meta AI's LLaMA 7B model and trained on 52K instruction-following demonstrations generated in the style of self-instruct using Open AI's text-davinci-003.

4. Dolly
Databricks' Dolly is an instruction-following large language model trained on the Databricks machine learning platform that is licensed for commercial use.
Dolly was trained on a much smaller language model of only six billion parameters versus 175 billion for GPT-3 (ChatGPT is fine-tuned on GPT-3.5).

5. Pythia
The Pythia Scaling Suite is a collection of models developed to facilitate interpretability research. It contains two sets of eight models of sizes 70M, 160M, 410M, 1B, 1.4B, 2.8B, 6.9B, and 12B.

6. GPT-NeoX
GPT-NeoX is the latest Natural Language Processing (NLP) model from EleutherAI. GPT-NeoX 20 is a text generation model, meaning that it can write text for you and actually achieve almost any Natural Language Processing use case with a great accuracy: blog post generation, chatbots, text classification, sentiment analysis, keywords extraction, code generation, entity extraction etc.

7. FLAN (Finetuned LAnguage Net)
Describes a method for improving zero-shot learning for Natural Language Processing (NLP) models by using natural language instructions (instruction tuning) by making use of pretraining, finetuning and prompting.
