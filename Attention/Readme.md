 Attention, which is a mechanism that allows models to focus on specific parts of the input during processing. What part should be given importance.
 
 1. Self-attention: 
    This type of attention mechanism allows a model to attend to different parts of the input sequence when making predictions. It is often used in tasks such as         language modeling and machine translation.

2. Global attention: 
   This type of attention mechanism allows a model to attend to all parts of the input sequence when making predictions. It is often used in tasks  such as sentiment    analysis and text classification.

3. Local attention: 
   This type of attention mechanism allows a model to attend to a specific part of the input sequence when making predictions. It is often used in  tasks such as        speech recognition and image captioning.

5. Multi-head attention: 
   This type of attention mechanism allows a model to attend to multiple parts of the input sequence simultaneously. It is often used in tasks such as question          answering and summarization.
   
5. Sparse attention 
   It is a type of attention mechanism used in natural language processing (NLP) that allows a model to attend to only a small subset of the input sequence when        making predictions. Unlike other attention mechanisms, which attend to all parts of the input sequence, sparse attention only attends to a few selected parts of      the sequence. This can be useful in cases where the input sequence is very long, as it allows the model to focus on the most relevant parts of the sequence and      ignore the rest.
Sparse attention can also be more computationally efficient than other attention mechanisms, as it requires fewer computations to attend to only a small subset of the input sequence. However, it can be more difficult to train, as the model must learn which parts of the sequence to attend to and which to ignore. Sparse attention has been used successfully in tasks such as machine translation and language modeling, and is an active area of research in the NLP community.
