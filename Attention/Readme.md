 Attention, which is a mechanism that allows models to focus on specific parts of the input during processing. What part should be given importance.
 
 1. Self-attention: 
    This type of attention mechanism allows a model to attend to different parts of the input sequence when making predictions. It is often used in tasks such as         language modeling and machine translation.

2. Global attention: 
   This type of attention mechanism allows a model to attend to all parts of the input sequence when making predictions. It is often used in tasks  such as sentiment    analysis and text classification.

3. Local attention: 
   This type of attention mechanism allows a model to attend to a specific part of the input sequence when making predictions. It is often used in  tasks such as        speech recognition and image captioning.

5. Multi-head attention: 
   This type of attention mechanism allows a model to attend to multiple parts of the input sequence simultaneously. It is often used in tasks such as question          answering and summarization.
